{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "022f229f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "310703f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1f0742",
   "metadata": {},
   "source": [
    "# Collecting Corresponding Points from images\n",
    "### we will use:\n",
    "- ginput(n)\n",
    "- numpy.save(path, var) - save variable into binary to path\n",
    "- numpy.load(path) - load that saved variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "605fd62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def pick_and_save_correspondences(im1_path, im2_path, n_corresp = 4, sample_name = 'temp' ):\n",
    "    # Create a figure with two subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(17, 14))\n",
    "# collecting correspondances\n",
    "    # Load or create two example images\n",
    "    img1 = mpimg.imread(im1_path)\n",
    "    img2 = mpimg.imread(im2_path)\n",
    "\n",
    "    ax1.imshow(img1)\n",
    "    ax1.set_title(\"Image 1\")\n",
    "\n",
    "    ax2.imshow(img2)\n",
    "    ax2.set_title(\"Image 2\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "\n",
    "    # Number of points to pick\n",
    "    pts1 = []\n",
    "    pts2 = []\n",
    "    correspondances = []\n",
    "    # Pick points alternately\n",
    "    for i in range(n_corresp):\n",
    "        # take point 1 at img1\n",
    "        print(f\"Select point {i+1} in Image 1\")\n",
    "        plt.sca(ax1)\n",
    "        p1 = plt.ginput(1)[0]\n",
    "        plt.plot(p1[0], p1[1], 'ro')\n",
    "        pts1.append(p1)\n",
    "\n",
    "        # take point 2 at img2\n",
    "        print(f\"Select corresponding point {i+1} in Image 2\")\n",
    "        plt.sca(ax2)\n",
    "        p2 = plt.ginput(1)[0]\n",
    "        plt.plot(p2[0], p2[1], 'go')\n",
    "        pts2.append(p2)\n",
    "\n",
    "        # push correspondance to list\n",
    "        correspondances.append((p1, p2))\n",
    "\n",
    "        # Draw lines showing correspondence across subplots\n",
    "        # Convert axes coordinates to normalised figure coordinates\n",
    "        fig.canvas.draw()\n",
    "        transFigure = fig.transFigure.inverted()\n",
    "        coord1 = ax1.transData.transform(p1)\n",
    "        coord2 = ax2.transData.transform(p2)\n",
    "        fig_coord1 = transFigure.transform(coord1)\n",
    "        fig_coord2 = transFigure.transform(coord2)\n",
    "        line = plt.Line2D((fig_coord1[0], fig_coord2[0]),\n",
    "                        (fig_coord1[1], fig_coord2[1]),\n",
    "                        transform=fig.transFigure,\n",
    "                        color='yellow', linestyle='--')\n",
    "        fig.lines.append(line)\n",
    "        plt.draw()\n",
    "    \n",
    "    plt.close()\n",
    "\n",
    "    # saving selected points\n",
    "    output_dir = f'./homography/{sample_name}/samples'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    output_fullpath = output_dir + f'/{n_corresp}_corresp'\n",
    "    np.save(output_fullpath, correspondances)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cc1eea",
   "metadata": {},
   "source": [
    "### Plotting funs used\n",
    "\n",
    ">We have **figure coordinate system (display)** that contains subplots, etc. (`fig`).\n",
    ">Every figure is like a **canvas**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 1ï¸âƒ£ `fig.transFigure.inverted()` vs `fig.transFigure`\n",
    "\n",
    "* **Display (figure) coordinate system** can be:\n",
    "\n",
    "1. **Normalized** `(0,0)` to `(1,1)`\n",
    "\n",
    "   * Independent of the figure size â†’ `fig.transFigure` (it's just a coordinate system)\n",
    "   * **MAPping:** `FIG.NORMAL_COORD â†’ FIG.PIXEL_COORD` â†’ `fig.transFigure.transform(coords)`\n",
    "   * **MAPping:** `FIG.PIXEL_COORD â†’ FIG.NORMAL_COORD` â†’ `fig.transFigure.inverted().transform(coords)`\n",
    "\n",
    "2. **In pixels**\n",
    "\n",
    "   * Shows actual pixels on the screen\n",
    "   * Depends on the size of the figure window (canvas) â†’ `fig.transFigure.inverted()` (it's just coord system for fig in pixels)\n",
    "\n",
    "---\n",
    "\n",
    "#### 2ï¸âƒ£ `ax_i.transData.transform(x, y)`\n",
    "\n",
    "1. `ax1.transData.transform(x, y)` maps **data points** `(x, y)` on subplot `ax1` to **pixels** in display coordinates (relative to the figure canvas).\n",
    "2. Similarly, `ax2.transData` does the same for subplot `ax2`.\n",
    "\n",
    "---\n",
    "\n",
    "#### 3ï¸âƒ£ Why do we map clicked points up until getting normalized figure coordinates?\n",
    "\n",
    "* Because we **cannot use `fig.transFigure.inverted` as the transform coordinate system** for drawing lines.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3950cd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select point 1 in Image 1\n",
      "Select corresponding point 1 in Image 2\n",
      "Select point 2 in Image 1\n",
      "Select corresponding point 2 in Image 2\n",
      "Select point 3 in Image 1\n",
      "Select corresponding point 3 in Image 2\n",
      "Select point 4 in Image 1\n",
      "Select corresponding point 4 in Image 2\n",
      "Select point 1 in Image 1\n",
      "Select corresponding point 1 in Image 2\n",
      "Select point 2 in Image 1\n",
      "Select corresponding point 2 in Image 2\n",
      "Select point 3 in Image 1\n",
      "Select corresponding point 3 in Image 2\n",
      "Select point 4 in Image 1\n",
      "Select corresponding point 4 in Image 2\n",
      "Select point 5 in Image 1\n",
      "Select corresponding point 5 in Image 2\n",
      "Select point 6 in Image 1\n",
      "Select corresponding point 6 in Image 2\n",
      "Select point 1 in Image 1\n",
      "Select corresponding point 1 in Image 2\n",
      "Select point 2 in Image 1\n",
      "Select corresponding point 2 in Image 2\n",
      "Select point 3 in Image 1\n",
      "Select corresponding point 3 in Image 2\n",
      "Select point 4 in Image 1\n",
      "Select corresponding point 4 in Image 2\n",
      "Select point 5 in Image 1\n",
      "Select corresponding point 5 in Image 2\n",
      "Select point 6 in Image 1\n",
      "Select corresponding point 6 in Image 2\n",
      "Select point 7 in Image 1\n",
      "Select corresponding point 7 in Image 2\n",
      "Select point 8 in Image 1\n",
      "Select corresponding point 8 in Image 2\n",
      "Select point 1 in Image 1\n",
      "Select corresponding point 1 in Image 2\n",
      "Select point 2 in Image 1\n",
      "Select corresponding point 2 in Image 2\n",
      "Select point 3 in Image 1\n",
      "Select corresponding point 3 in Image 2\n",
      "Select point 4 in Image 1\n",
      "Select corresponding point 4 in Image 2\n",
      "Select point 5 in Image 1\n",
      "Select corresponding point 5 in Image 2\n",
      "Select point 6 in Image 1\n",
      "Select corresponding point 6 in Image 2\n",
      "Select point 7 in Image 1\n",
      "Select corresponding point 7 in Image 2\n",
      "Select point 8 in Image 1\n",
      "Select corresponding point 8 in Image 2\n",
      "Select point 9 in Image 1\n",
      "Select corresponding point 9 in Image 2\n",
      "Select point 10 in Image 1\n",
      "Select corresponding point 10 in Image 2\n",
      "Select point 1 in Image 1\n",
      "Select corresponding point 1 in Image 2\n",
      "Select point 2 in Image 1\n",
      "Select corresponding point 2 in Image 2\n",
      "Select point 3 in Image 1\n",
      "Select corresponding point 3 in Image 2\n",
      "Select point 4 in Image 1\n",
      "Select corresponding point 4 in Image 2\n",
      "Select point 5 in Image 1\n",
      "Select corresponding point 5 in Image 2\n",
      "Select point 6 in Image 1\n",
      "Select corresponding point 6 in Image 2\n",
      "Select point 7 in Image 1\n",
      "Select corresponding point 7 in Image 2\n",
      "Select point 8 in Image 1\n",
      "Select corresponding point 8 in Image 2\n",
      "Select point 9 in Image 1\n",
      "Select corresponding point 9 in Image 2\n",
      "Select point 10 in Image 1\n",
      "Select corresponding point 10 in Image 2\n",
      "Select point 11 in Image 1\n",
      "Select corresponding point 11 in Image 2\n",
      "Select point 12 in Image 1\n",
      "Select corresponding point 12 in Image 2\n",
      "Select point 1 in Image 1\n",
      "Select corresponding point 1 in Image 2\n",
      "Select point 2 in Image 1\n",
      "Select corresponding point 2 in Image 2\n",
      "Select point 3 in Image 1\n",
      "Select corresponding point 3 in Image 2\n",
      "Select point 4 in Image 1\n",
      "Select corresponding point 4 in Image 2\n",
      "Select point 5 in Image 1\n",
      "Select corresponding point 5 in Image 2\n",
      "Select point 6 in Image 1\n",
      "Select corresponding point 6 in Image 2\n",
      "Select point 7 in Image 1\n",
      "Select corresponding point 7 in Image 2\n",
      "Select point 8 in Image 1\n",
      "Select corresponding point 8 in Image 2\n",
      "Select point 9 in Image 1\n",
      "Select corresponding point 9 in Image 2\n",
      "Select point 10 in Image 1\n",
      "Select corresponding point 10 in Image 2\n",
      "Select point 11 in Image 1\n",
      "Select corresponding point 11 in Image 2\n",
      "Select point 12 in Image 1\n",
      "Select corresponding point 12 in Image 2\n",
      "Select point 13 in Image 1\n",
      "Select corresponding point 13 in Image 2\n",
      "Select point 14 in Image 1\n",
      "Select corresponding point 14 in Image 2\n",
      "Select point 15 in Image 1\n",
      "Select corresponding point 15 in Image 2\n"
     ]
    }
   ],
   "source": [
    "# collecting correspondances\n",
    "im1_path = './images/paris/paris_c.jpg'\n",
    "im2_path = './images/paris/paris_b.jpg' # (BASE IMG)\n",
    "\n",
    "#result_path = result_root_dir + os.path.basename(im1_path)\n",
    "for sample_size in [4,6,8,10, 12, 15]:\n",
    "    pick_and_save_correspondences(im1_path, im2_path, sample_size, sample_name='paris/H_cb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b197041",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_path = './homography/cmpe_building/left1_left2/samples/4_corresp.npy'\n",
    "\n",
    "def homography_by_sample(sample_path):\n",
    "    # load saved sample info\n",
    "    points_data = np.load(sample_path)\n",
    "\n",
    "    # compute homography\n",
    "    points_im1 = points_data[:, 0]\n",
    "    points_im2 = points_data[:, 1]\n",
    "\n",
    "    return computeH(points_im1, points_im2)\n",
    "\n",
    "def computeH(points_im1, points_im2):\n",
    "\n",
    "    # constructing CALIBRATION MATRIX\n",
    "    X1 = points_im1[:, 0]\n",
    "    Y1 = points_im1[:, 1]\n",
    "\n",
    "    ones = np.ones_like(X1)\n",
    "    zeros = np.zeros_like(X1)\n",
    "\n",
    "    X2 = points_im2[:, 0]\n",
    "    Y2 = points_im2[:, 1]\n",
    "\n",
    "    # last 3 columns of rows of type-1\n",
    "    NX2X1 = -1 * X2 * X1\n",
    "    NX2Y1 = -1 * X2 * Y1\n",
    "    NX2 = -1 * X2\n",
    "\n",
    "    # last 3 columns of other rows\n",
    "    NY2X1 = -1 * Y2 * X1\n",
    "    NY2Y1 = -1 * Y2 * Y1\n",
    "    NY2 = -1 * Y2\n",
    "\n",
    "    row1 = np.stack([X1, Y1, ones, zeros, zeros, zeros, NX2X1, NX2Y1, NX2], axis = 1)\n",
    "    row2 = np.stack([zeros, zeros, zeros, X1, Y1, ones, NY2X1, NY2Y1, NY2], axis = 1)\n",
    "\n",
    "    A_matrix = np.vstack([row1, row2])\n",
    "\n",
    "    # compute svd for minimisation problem: h = argmin_h(Ah)\n",
    "    U, S, Vt = np.linalg.svd(A_matrix)\n",
    "    # solution is last row of V\n",
    "    h = Vt[-1, :]\n",
    "    H = h.reshape(3,3)\n",
    "\n",
    "    # normalised solution to right-bottom corner element\n",
    "    H = (H / H[2,2])\n",
    "\n",
    "    return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "42236e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.77659417e-01 -7.56560787e-02  3.54225652e+02]\n",
      " [-1.12237382e-01  7.56285536e-01  8.90783837e+01]\n",
      " [-2.77296245e-04 -5.94592452e-05  1.00000000e+00]]\n",
      "[[ 8.02261996e-01 -1.77854556e-02  2.68929237e+02]\n",
      " [-7.32107906e-02  9.08446409e-01  4.36167700e+01]\n",
      " [-1.62868120e-04 -1.37430832e-05  1.00000000e+00]]\n",
      "[[ 7.53844199e-01 -4.02178133e-04  2.79165644e+02]\n",
      " [-7.99800681e-02  8.90470781e-01  4.83669598e+01]\n",
      " [-1.95611409e-04  1.22147475e-06  1.00000000e+00]]\n",
      "[[ 7.28836195e-01 -2.06233652e-02  2.95480719e+02]\n",
      " [-8.58860833e-02  8.72984640e-01  5.53838440e+01]\n",
      " [-2.01520192e-04 -1.46467713e-05  1.00000000e+00]]\n",
      "[[ 7.81663710e-01 -2.18801081e-02  2.71836733e+02]\n",
      " [-7.19569774e-02  8.97486036e-01  4.06820023e+01]\n",
      " [-1.74722084e-04 -2.43652027e-05  1.00000000e+00]]\n",
      "[[ 7.68419393e-01 -4.08229185e-02  2.83013409e+02]\n",
      " [-7.48318135e-02  8.81721867e-01  4.66858606e+01]\n",
      " [-1.76505839e-04 -3.78310602e-05  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "sample_sizes = [4, 6, 8, 10, 12, 15]\n",
    "\n",
    "for sample_size in sample_sizes:\n",
    "    sample_path = f'./homography/cmpe_building/left1_left2/samples/{sample_size}_corresp.npy'\n",
    "    print(homography_by_sample(sample_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0267552f",
   "metadata": {},
   "source": [
    "## ðŸŒ€ Warping Image\n",
    "\n",
    "### 0) Why not Forward Warping?\n",
    "\n",
    "We **donâ€™t use forward warping** because itâ€™s inefficient and produces artifacts.\n",
    "Since real images contain **noise** and **picked correspondence points arenâ€™t perfectly accurate**,\n",
    "some points in `img1` may get mapped to the **same pixel** in `img2` plane by the homography $ H_{12} $.\n",
    "\n",
    "As a result, when we compute\n",
    "\n",
    "$\n",
    "(x', y', 1)^T = H_{12} \\cdot (x, y, 1)^T\n",
    "$\n",
    "\n",
    "the set of mapped pixels ${(x', y')}$ may **not cover every pixel** in the target image â€”\n",
    "causing **holes** (unfilled regions) where no source pixel lands.\n",
    "\n",
    "---\n",
    "\n",
    "### 1) Backward Warping Idea\n",
    "\n",
    "The idea is simple:\n",
    "we **invert the mapping** and instead ask:\n",
    "\n",
    "> â€œFor each pixel (xâ€², yâ€²) in the output image (img2 plane),\n",
    "> from which location (x, y) in img1 should its color come?â€\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "$\n",
    "\\text{color}(x', y') = \\text{color}*{\\text{img1}}(H*{12}^{-1} \\cdot (x', y', 1)^T)\n",
    "$\n",
    "\n",
    "So for every output pixel, we **trace it backward** through $ H_{12}^{-1} $\n",
    "to find where it came from in `img1`.\n",
    "\n",
    "---\n",
    "\n",
    "### 1.1) Constructing All (xâ€², yâ€², 1) Points\n",
    "\n",
    "To apply $ H_{12}^{-1} $ to **all output pixels at once**,\n",
    "we create a grid of all pixel coordinates on the output (img2) plane.\n",
    "\n",
    "We use **`np.meshgrid`** to generate all $(xâ€², yâ€²)$ combinations:\n",
    "\n",
    "Perfect â€” hereâ€™s a clear explanation (with math) of **how `np.meshgrid` builds the full coordinate matrix** used in backward warping ðŸ‘‡\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ§© Constructing the Coordinate Matrix with `np.meshgrid`\n",
    "\n",
    "Letâ€™s say your **output image** (the plane we warp onto) has size:\n",
    "\n",
    "$\n",
    "\\text{height} = h_{out}, \\quad \\text{width} = w_{out}\n",
    "$\n",
    "\n",
    "So, pixel coordinates on this image are integer grid points:\n",
    "\n",
    "$\n",
    "x' \\in [0, w_{out}-1], \\quad y' \\in [0, h_{out}-1]\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### 1ï¸âƒ£ Build coordinate grids\n",
    "\n",
    "We use:\n",
    "\n",
    "```python\n",
    "xs, ys = np.meshgrid(np.arange(w_out), np.arange(h_out))\n",
    "```\n",
    "\n",
    "* `np.arange(w_out)` â†’ `[0, 1, 2, ..., w_out-1]`  â†’ **x-coordinates**\n",
    "* `np.arange(h_out)` â†’ `[0, 1, 2, ..., h_out-1]` â†’ **y-coordinates**\n",
    "* `meshgrid` repeats them to form **full 2D coordinate grids**:\n",
    "\n",
    "Example (for `w_out=3`, `h_out=2`):\n",
    "\n",
    "```python\n",
    "xs =\n",
    "[[0, 1, 2],\n",
    " [0, 1, 2]]\n",
    "\n",
    "ys =\n",
    "[[0, 0, 0],\n",
    " [1, 1, 1]]\n",
    "```\n",
    "\n",
    "That means:\n",
    "\n",
    "* first row of `xs, ys` corresponds to y=0 (top row of image)\n",
    "* second row corresponds to y=1, etc.\n",
    "Together they define a mesh of coordinate pairs:\n",
    "\n",
    "```scss\n",
    "    (0,0) (1,0) (2,0)\n",
    "    (0,1) (1,1) (2,1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ Flatten into column form\n",
    "\n",
    "We flatten both to 1D arrays:\n",
    "\n",
    "```python\n",
    "x_flat = xs.ravel()\n",
    "y_flat = ys.ravel()\n",
    "```\n",
    "\n",
    "Result:\n",
    "\n",
    "$\n",
    "x_{flat} = [0, 1, 2, 0, 1, 2]\n",
    "$\n",
    "$\n",
    "y_{flat} = [0, 0, 0, 1, 1, 1]\n",
    "$\n",
    "\n",
    "Each pair ((x_i', y_i')) corresponds to **one pixel** in the output image.\n",
    "\n",
    "---\n",
    "\n",
    "### 3ï¸âƒ£ Convert to homogeneous coordinates\n",
    "\n",
    "Add a third coordinate = 1 for all points:\n",
    "\n",
    "```python\n",
    "ones = np.ones_like(x_flat)\n",
    "```\n",
    "\n",
    "Then **stack** them as rows (axis=0):\n",
    "\n",
    "```python\n",
    "p2_h = np.stack([x_flat, y_flat, ones], axis=0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4ï¸âƒ£ Resulting matrix shape\n",
    "\n",
    "`p2_h` is a **3Ã—N matrix**, where $ N = h_{out} \\times w_{out} $.\n",
    "\n",
    "$\n",
    "p_2^h =\n",
    "\\begin{bmatrix}\n",
    "x'_1 & x'_2 & \\dots & x'_N \\\\\n",
    "y'_1 & y'_2 & \\dots & y'_N \\\\\n",
    "1 & 1 & \\dots & 1\n",
    "\\end{bmatrix}\n",
    "$\n",
    "\n",
    "Each column corresponds to **where img2 pixel comes from, if it was on plane img1**.\n",
    "---\n",
    "\n",
    "### 5ï¸âƒ£ Why this form?\n",
    "\n",
    "This form allows us to apply the inverse homography to *all pixels at once* using matrix multiplication:\n",
    "\n",
    "$\n",
    "p_1^h = H_{12}^{-1} \\cdot p_2^h\n",
    "$\n",
    "\n",
    "which efficiently gives the mapped coordinates ((x, y)) in the source image for **every output pixel**.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## Interpolating and Color setting at once\n",
    "\n",
    "### 1ï¸âƒ£ Ideal mathematical model (no noise, integer coordinates)\n",
    "\n",
    "If your homography were **perfect** and pixel-aligned, then for each pixel ((x',y')) on the **output image (img2 plane)**,\n",
    "youâ€™d find its corresponding pixel ((x,y)) on **img1 plane** as:\n",
    "\n",
    "$\n",
    "[x, y, 1]^T ;=; H^{-1} [x', y', 1]^T\n",
    "$\n",
    "\n",
    "and ideally, those ((x,y)) would be **integers** (exact pixel centers).\n",
    "So, the color mapping is:\n",
    "\n",
    "$\n",
    "\\text{color}*{\\text{out}}(x',y') ;=; \\text{color}*{\\text{img1}}(x,y)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### 2ï¸âƒ£ Real-world case (noise, non-integer mappings) - OUR CASE\n",
    "\n",
    "To get the color, we interpolate between the four neighboring pixels around ((x,y)):\n",
    "\n",
    "$\n",
    "\\text{color}_{\\text{img2}}(x',y') = \\text{color}_{\\text{img1}}\\big( \\text{Interpolation}(H^{-1} [x', y', 1]^T) \\big)\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ” Intuitive explanation\n",
    "\n",
    "* $H^{-1}$ tells **where this pixel on the output came from** on img1.\n",
    "* That source coordinate may not be an integer pixel â†’ so we **interpolate** color from nearby pixels in img1.\n",
    "* Thatâ€™s why we say â€œbackward warpingâ€:\n",
    "  we go **back** from output â†’ input to **sample** colors smoothly.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f88aba26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp(image, homography, output_shape=None):\n",
    "    h_in, w_in = image.shape[:2]\n",
    "\n",
    "    if output_shape is None:\n",
    "        # get sizes of image (it's img1)\n",
    "        height = image.shape[0]\n",
    "        width = image.shape[1]\n",
    "    else:\n",
    "        height, width = output_shape\n",
    "\n",
    "    # get matrix of all points as columns\n",
    "    x_axis = np.arange(width)\n",
    "    y_axis = np.arange(height)\n",
    "\n",
    "    # rows we will stack for the matrix\n",
    "    x_mesh, y_mesh = np.meshgrid(x_axis, y_axis)\n",
    "    ones = np.ones_like(np.ravel(x_mesh))\n",
    "    \n",
    "    # A is matrix of all points in homogeneos coord.sys for plane 2\n",
    "    P2 = np.stack([np.ravel(x_mesh), np.ravel(y_mesh), ones], axis = 0)\n",
    "\n",
    "    # now, we need to get colors of them by backward transform\n",
    "    P21 = np.dot(np.linalg.inv(homography), P2 )\n",
    "    P21 = P21 / P21[2,:]\n",
    "    \n",
    "    # interpolating with opencv function\n",
    "    \n",
    "    # 1. get coordinate maps\n",
    "    x_map = P21[0, :].reshape(height, width).astype(np.float32)\n",
    "    y_map = P21[1, :].reshape(height, width).astype(np.float32)\n",
    "\n",
    "    # 2. using interpolation function opencv that also makes mapping of colors\n",
    "    warped_image = cv2.remap(image, x_map, y_map ,interpolation = cv2.INTER_LINEAR, borderMode = cv2.BORDER_CONSTANT, borderValue=0)\n",
    "    \n",
    "    # return interpolated and remaped \n",
    "    return warped_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cbb86a9",
   "metadata": {},
   "source": [
    "## Making Warped Part Visible â€” `warp_full`\n",
    "\n",
    "1. Apply transform so all warped corners have positive coordinates:  \n",
    "   $$\n",
    "   H_{\\text{adjusted}} = \n",
    "   \\begin{bmatrix}\n",
    "   1 & 0 & -x_{\\min}\\\\\n",
    "   0 & 1 & -y_{\\min}\\\\\n",
    "   0 & 0 & 1\n",
    "   \\end{bmatrix} H\n",
    "   $$\n",
    "   ensuring $ H_{\\text{adjusted}}^{-1}(x_b, y_b, 1) = (x, y, 1) $ lies in image grid.  \n",
    "\n",
    "2. Set output size to  \n",
    "   $$\n",
    "      W = \\lceil x_{\\max} - x_{\\min} \\rceil, \\quad \n",
    "      H = \\lceil y_{\\max} - y_{\\min} \\rceil,\n",
    "   $$\n",
    "   so all backward-transformed points fit within the visible output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a29cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def warp_full(image, homography):\n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    # Find corners and warp them\n",
    "    corners = np.array([[0, 0, 1],\n",
    "                        [w, 0, 1],\n",
    "                        [w, h, 1],\n",
    "                        [0, h, 1]]).T\n",
    "    warped_corners = homography @ corners\n",
    "    warped_corners /= warped_corners[2, :]\n",
    "    xs, ys = warped_corners[0], warped_corners[1]\n",
    "\n",
    "    # Translate original corners to BASE img. plane\n",
    "    xmin, xmax = xs.min(), xs.max()\n",
    "    ymin, ymax = ys.min(), ys.max()\n",
    "\n",
    "    # Get Translation matrix, making adjusted Homography\n",
    "    trans = np.array([[1, 0, -xmin],\n",
    "                      [0, 1, -ymin],\n",
    "                      [0, 0, 1]])\n",
    "    \n",
    "    # H_adj_inverse will now, make base coord. to map at coorners of original img.\n",
    "    # So that in Backward Transform. we will not have points outside original image plane.\n",
    "    H_adj = trans @ homography\n",
    "\n",
    "    out_w, out_h = int(np.ceil(xmax - xmin)), int(np.ceil(ymax - ymin))\n",
    "    return warp(image, H_adj, (out_h, out_w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0518bf23",
   "metadata": {},
   "source": [
    "## Task Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9a4505",
   "metadata": {},
   "source": [
    "### Task 1 â€” Paris\n",
    "\n",
    "Steps:  \n",
    "- [ ] (1) Compute homography \\(H\\) using correspondences: **paris_a â†’ paris_b (base)**.  \n",
    "- [ ] (2) Warp and stitch **{paris_a, paris_b, paris_c}** onto the **paris_b** base frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd256d2",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "matmul: Input operand 0 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m     homograpgy = homography_by_sample(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m./homography/paris/H_cb/samples/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_corresp.npy\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# warping with img size fitting\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m warped_img = \u001b[43mwarp_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhomograpgy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# saving result\u001b[39;00m\n\u001b[32m     23\u001b[39m result_save_path = base_path + \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moriginal_img_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/samples/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_corresp/\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mwarp_full\u001b[39m\u001b[34m(image, homography)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Find corners and warp them\u001b[39;00m\n\u001b[32m      5\u001b[39m corners = np.array([[\u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m],\n\u001b[32m      6\u001b[39m                     [w, \u001b[32m0\u001b[39m, \u001b[32m1\u001b[39m],\n\u001b[32m      7\u001b[39m                     [w, h, \u001b[32m1\u001b[39m],\n\u001b[32m      8\u001b[39m                     [\u001b[32m0\u001b[39m, h, \u001b[32m1\u001b[39m]]).T\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m warped_corners = \u001b[43mhomography\u001b[49m\u001b[43m \u001b[49m\u001b[43m@\u001b[49m\u001b[43m \u001b[49m\u001b[43mcorners\u001b[49m\n\u001b[32m     10\u001b[39m warped_corners /= warped_corners[\u001b[32m2\u001b[39m, :]\n\u001b[32m     11\u001b[39m xs, ys = warped_corners[\u001b[32m0\u001b[39m], warped_corners[\u001b[32m1\u001b[39m]\n",
      "\u001b[31mValueError\u001b[39m: matmul: Input operand 0 does not have enough dimensions (has 0, gufunc core with signature (n?,k),(k,m?)->(n?,m?) requires 1)"
     ]
    }
   ],
   "source": [
    "# 1. Stitching Paris Images, with paris_b as base.\n",
    "base_path =  './tasks/1'\n",
    "\n",
    "\n",
    "img_paris_a = mpimg.imread('./images/paris/paris_a.jpg')\n",
    "img_paris_b = mpimg.imread('./images/paris/paris_b.jpg')\n",
    "img_paris_c = mpimg.imread('./images/paris/paris_c.jpg')\n",
    "\n",
    "# warping for all H_cb samples\n",
    "for sample_size in [4, 6, 8, 10, 12, 15]:\n",
    "    for original_img, original_img_name in [[img_paris_a, 'paris_a'], [img_paris_b, 'paris_b']]:\n",
    "        # getting calculated homographies by sample_size (we pick 15)\n",
    "        if original_img_name == 'paris_a':\n",
    "            homography = homography_by_sample(f'./homography/paris/H_ab/samples/{sample_size}_corresp.npy')\n",
    "        else:\n",
    "            homography = homography_by_sample(f'./homography/paris/H_cb/samples/{sample_size}_corresp.npy')\n",
    "        \n",
    "        # warping with img size fitting\n",
    "        warped_img = warp_full(original_img, homography)\n",
    "\n",
    "        # saving result\n",
    "        result_save_path = base_path + f'/{original_img_name}/samples/{sample_size}_corresp/'\n",
    "        os.makedirs(result_save_path, exist_ok=True )\n",
    "        mpimg.imsave(result_save_path + f'warped_{original_img_name}(base_b).jpg', warped_img)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c819007d",
   "metadata": {},
   "source": [
    "## Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21755dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "for building_name in ['cmpe_building', 'north_campus']:\n",
    "    # build full dir-path\n",
    "    base_path = './tasks/2/{building_name}'\n",
    "\n",
    "    #2.1 left-to-right\n",
    "    img_dir = f'{base_path}/left_to_right/'\n",
    "\n",
    "    #2.2 middle out\n",
    "    img_dir = f'{base_path}/middle_out/'\n",
    "\n",
    "    #2.3 first_out_then_middle\n",
    "    img_dir = f'{base_path}/first_out_then_middle/'\n",
    "\n",
    "    # Stitching Image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
